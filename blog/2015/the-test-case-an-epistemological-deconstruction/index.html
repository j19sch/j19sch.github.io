<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>The test case - an epistemological deconstruction | Joep Schuurkes</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="alternate" type="application/atom+xml" title="Atom" hreflang="en" href="../../feed.atom">
<link rel="canonical" href="https://smallsheds.garden/blog/2015/the-test-case-an-epistemological-deconstruction/">
<link rel="icon" href="../../../assets/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" href="../../../assets/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" href="../../../assets/favicons/favicon-96x96.png" sizes="96x96">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Privacy-friendly analytics by Plausible --><script async src="https://plausible.io/js/pa-QXPguu2aya7fgEtw5-p4P.js"></script><script>
  window.plausible=window.plausible||function(){(plausible.q=plausible.q||[]).push(arguments)},plausible.init=plausible.init||function(i){plausible.o=i||{}};
  plausible.init()
</script><meta name="author" content="Joep Schuurkes">
<link rel="prev" href="../../2014/joining-the-fray-on-iso-29119/" title="Joining the fray on ISO 29119" type="text/html">
<link rel="next" href="../test-automation-five-questions-leading-to-five-heuristics/" title="Test automation - five questions leading to five heuristics" type="text/html">
<meta property="og:site_name" content="Joep Schuurkes">
<meta property="og:title" content="The test case - an epistemological deconstruction">
<meta property="og:url" content="https://smallsheds.garden/blog/2015/the-test-case-an-epistemological-deconstruction/">
<meta property="og:description" content="(This article was first published in Dutch in TestNet Nieuws 18. The article below is a translation with minor changes. Many thanks to Joris Meerts and Ruud Cox for reviewing the original version.)
Te">
<meta property="og:image" content="https://smallsheds.garden/images/default-preview.jpeg">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2015-02-01T22:19:55+01:00">
<meta property="article:tag" content="epistemology">
<meta property="article:tag" content="information debt">
<meta property="article:tag" content="test cases">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../../">

            <span id="blog-title">Joep Schuurkes</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav"></ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item dropdown">
<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Blog</a>
            <div class="dropdown-menu">
                    <a href="../../" class="dropdown-item">My blog</a>
                    <a href="../../../categories/" class="dropdown-item">Categories and tags</a>
                    <a href="../../../archive.html" class="dropdown-item">Archive</a>
                    <a href="../../rss.xml" class="dropdown-item"><span class="caps">RSS</span> feed</a>
                    <a href="../../feed.atom" class="dropdown-item">Atom feed</a>
            </div>
                </li>
<li class="nav-item">
<a href="../../../my-projects/" class="nav-link">Projects</a>
                </li>
<li class="nav-item">
<a href="../../../my-talks/" class="nav-link">Talks</a>
                </li>
<li class="nav-item">
<a href="../../../index.html" class="nav-link">About&nbsp;me</a>

                
            </li>
</ul>
<div id="search" style="text-align: center;margin-left: 40px; margin-right: 40px">
                <form method="get" id="search" target="_blank" action="https://noai.duckduckgo.com/">
                    <input type="hidden" name="sites" value="https://smallsheds.garden/"><input type="hidden" name="k8" value="#444444"><!-- text color code --><input type="hidden" name="k9" value="#D51920"><!-- links color code --><input type="hidden" name="kt" value="h"><!-- text font --><input type="text" name="q" size="14" maxlength="255" placeholder="Search..." arial-label="search this site" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="display:none;">
</form>
            </div>
        </div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">The test case - an epistemological&nbsp;deconstruction</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Joep&nbsp;Schuurkes
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2015-02-01T22:19:55+01:00" itemprop="datePublished" title="1 February 2015">1 February 2015</time></a>
            </p>
            
        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p><em>(This article was first published in Dutch in <a href="https://www.testnet.org/testnet/home">TestNet</a> Nieuws 18. The article below is a translation with minor changes. Many thanks to Joris Meerts and Ruud Cox for reviewing the original&nbsp;version.)</em></p>
<h3>Testing as an information&nbsp;problem</h3>
<p>Testing is an information problem. We are in search of certain information, of an answer to the question: does this application fulfill the relevant explicit and implicit expectations? The exact way in which we can answer this question, however, is not immediately clear. First we will need to decide which questions to ask, how to ask them and how to evaluate the responses. Hence, testing is an information&nbsp;problem.</p>
<p>For the traditional test methodologies (<span class="caps">ISTQB</span> and TMap being the most well-known) the test case is a large part of the solution. So let&#8217;s take this solution apart epistemologically and see what it is we have in front of us. If the traditional test case is our solution, what information does a test case contain? What changes occur after executing it? And also, where is the understanding in all of this that&#8217;s&nbsp;happening?</p>
<p>In this article, I will first describe how a typical test case is created and how it is used. Then we shall take a look at which kinds of information a test case contains. Finally, we will analyze where the understanding of what happens during testing, is present and where it is&nbsp;not.</p>
<!-- TEASER_END -->

<h3>Creation and usage of the test&nbsp;case</h3>
<p>To begin with let&#8217;s find out what the traditional test methodologies have to say about creating and using test cases. Because of the philosophical nature of this article, I will only look at what these methodologies describe and ignore possible pragmatic&nbsp;deviations.</p>
<h4>Test&nbsp;basis</h4>
<p>A test case is created starting from the test basis. In the test basis the expectations with regards to the application are documented. Most likely not all (but close to all) explicit expectations are present. And note that some of these explicit expectations have only become so during the documentation&nbsp;process.</p>
<p>Besides the explicit expectations the test basis also contains a number of implicit expectations: expectations of which you can deduce the existence based on the explicit expectations present in the test basis. As a consequence the implicit expectations in the test basis will deviate from the &#8216;actual&#8217; implicit expectations, for they are based on a different set of explicit expectations. To summarize, the test basis is not a copy, but a model of the expectations of the&nbsp;application.</p>
<div class="d-flex justify-content-center">
    <figure class="figure"><img src="../../../images/2015/test-case-deconstruction/e-ts-tb-tdt.jpg" class="figure-img img-fluid rounded" alt="diagram: expectations, test strategy, test basis, test design technique"></figure>
</div>

<h4>Test design&nbsp;technique</h4>
<p>To create a test case one uses the test design techniques selected in the test strategy. Like the test basis, the test strategy is a transformation, a model of the explicit and implicit expectations of the application. While this is a fairly straightforward transformation in the case of the test basis (documenting expectations), it&#8217;s more complex for the test strategy. Besides expectations about the behaviour of the application, the test strategy also takes risks into&nbsp;account.</p>
<p>The combination of these two models (test basis and test strategy) by means of test design techniques results in the third model: the collection of test cases. Obviously the same applies here as with the test basis: there will not be a one-to-one relation between the test cases and the actual expectations about the application. Even more, there won&#8217;t be a one-to-one relation between the expectations documented by the test basis and the expectations documented by the test cases. Some information will be lost, some will be gained. It would be interesting to explore how all these elements (actual expectations, test basis, test strategy and set of test cases) influence each other, but unfortunately I have to leave that out of scope for this&nbsp;article.</p>
<h4>Test coverage&nbsp;matrix</h4>
<p>One test design technique - and I hope we&#8217;re using more than one - results in multiple test cases. Most of the time we group these test cases into for instance a test script to make test execution easier. This makes it difficult to keep track to which part (or parts) of the test basis each test case relates. The solution to this is creating a coverage matrix (aka traceability matrix): a table that documents these&nbsp;relations.</p>
<h4>Test&nbsp;case</h4>
<p>A test case consists of two parts: on the one hand input (test data and steps to be executed) and preconditions, on the other expected output and postconditions. More precise would be to say &#8220;expected input and preconditions&#8221;. Setting aside the question if the executing tester correctly identifies the preconditions and correctly enters the input, there is the fact that it&#8217;s no more than an expectation of ours that it&#8217;s possible to enter the specific input of the test case under the preconditions described in the test case. Until we actually try to execute the test case and see that it can be done, it is no more than that, an expectation. The same applies to the processing of the input by the application. Hence the wavy lines in the&nbsp;illustration.</p>
<p>A test case is our expectation based on the best knowledge we have when creating the test case, but that knowledge has not been tested yet. We don&#8217;t truly know anything about the application we are planning to test, until we actually test&nbsp;it.</p>
<div class="d-flex justify-content-center">
    <figure class="figure"><img src="../../../images/2015/test-case-deconstruction/testcase.jpg" class="figure-img img-fluid rounded" alt="diagram: terst design, test execution"></figure>
</div>

<h4>Test&nbsp;result</h4>
<p>When we execute the test case, we check the preconditions, enter the input and compare the actual output with the expected output and postconditions. Based on that we decide: &#8216;pass&#8217; or &#8216;fail&#8217;. This moment is the first time the expectations that lead to a to-be-tested application come into direct contact with the expectations that lead to a set of test cases. The result is documented in these test cases as a series of green checks and red crosses, a series of passes and&nbsp;fails.</p>
<h3>Types of information in a test&nbsp;case</h3>
<p>Now we have described what a test case is (a possible solution to an information problem), it&#8217;s time to look at what information is present in a test case. We can distinguish the following four types (indicated by black numbers in the illustration): 1. How the application is supposed to work; 2. How the application actually behaves; 3. Why this test case was created; 4. What has been tested. Let&#8217;s go over these one by&nbsp;one.</p>
<h4>How the application is supposed to&nbsp;work</h4>
<p>The information on how the application is supposed to work is present in the test case as such: the input, the expected output, the preconditions, the postconditions. As said earlier it&#8217;s important to realize that when we create the test case, we don&#8217;t know yet how the application actually behaves. We work based on expectations, also when determining the input and the preconditions. Of some expectations we are quite certain, of others less&nbsp;so.</p>
<p>This results in an interesting tension within the expectations about how the application is supposed to work: at what moment are you certain enough of a particular expectation to accept it as input and/or precondition of a test case? Another question is what information is lost when transforming the test basis by means of a test design technique to a number of test cases. We lose the implicit expectations present in the test basis in exchange for the implicit expectations present in the test&nbsp;cases.</p>
<p>This exactly is both the strength and the weakness of test design techniques: they allow us to hone in on certain specific expectations; that there is also a loss in information we just have to accept. Another thing we lose in this transformation is the structure of the test basis, the relations between its parts. Often we try to compensate this loss with a coverage matrix: how does the structure of the test basis relate to the structure of the test&nbsp;cases?</p>
<div class="d-flex justify-content-center">
    <figure class="figure"><img src="../../../images/2015/test-case-deconstruction/testcase_epsitemdeconstr.jpg" class="figure-img img-fluid rounded" alt="diagram: terst design, test execution"></figure>
</div>

<h4>How the application actually&nbsp;behaves</h4>
<p>During test execution we begin to discover how the application actually behaves. The expectations are tested against the application. One way to think about what happens is by means of John Boyd&#8217;s <span class="caps">OODA</span>-loop: Observe - Orient - Decide - Act. We execute the test case and go through each of the four phases: we see the output (Observe), we interpret our observations (Orient), on which we base our evaluation (Decide) and finally we do something (Act). (see&nbsp;illustration)</p>
<p>For a test case the evaluation is all about the question: Is there a problem here? Does the output conform to the expected output or not? Since the test case describes the expected output, it also is the oracle, the mechanism based on which we decide if there is a problem or not. The test case describes what you should expect to see as output; if you don&#8217;t see it, there&#8217;s a&nbsp;problem.</p>
<p>The thought processes of the tester during test execution - how we observe, how we orient, which decision we take - are thus for a large part determined in advance by the test case we have in front of us. Even more, the <span class="caps">OODA</span>-loop is not really a loop. After executing a test case, the tester will not go through an <span class="caps">OODA</span>-loop to determine which test case is to be executed next. The next case has been prepared already, it&#8217;s simply the next one on the&nbsp;stack.</p>
<h4>Why this test&nbsp;case</h4>
<p>Each test case exists for a reason. It was created because the test strategy determined a certain test design technique needed to be applied on the test basis. Or put differently, if we think of strategy/tactics/operations (see illustration), it&#8217;s the test strategy that describes the strategic level of our testing. The tactical level, however, which connects the test strategy with the actual testing, isn&#8217;t described explicitly anywhere. It&#8217;s hidden in our choice of test design techniques. The test operations, finally, are described in our test cases. This means that the reason of existence for a particular test case isn&#8217;t described or documented explicitly. We have to actively interpret the test case, the test design technique and the test strategy to reconstruct that reason. The big question here is how closely this reconstruction resembles the original&nbsp;reasoning.</p>
<h4>What has been&nbsp;tested</h4>
<p>The question of what has been tested, can be asked on several different levels. On the level of the test case this question can be answered fairly easily: a test case has been fully executed or not, it passed or it failed. Answering this question on a higher level immediately becomes much more difficult. As just mentioned, the test tactics are not described explicitly. To get to the strategy we will have to make that leap ourselves. That leap as such can be made, but it prevents us from talking about the test strategy on a different level than either the details of the test cases or the abstraction of the test strategy. There is nothing in&nbsp;between.</p>
<p>A possible solution is to use a test coverage matrix. However, it&#8217;s a limited solution. In the end this matrix does nothing more than link the expectations from the test cases to the expectations from the test basis. Although that does give us another angle, it does not bring our thinking to another&nbsp;level.</p>
<p>So the gain is limited. So both of these approaches (linking test cases to either test strategy or to test basis) bring along their own share of problems. Perhaps that&#8217;s why there is a third and easier solution: having faith in the work that has been done&nbsp;earlier.</p>
<h4>Where is the&nbsp;understanding?</h4>
<p>If we now take a step back to get a good overview, one thing that stands out is the dispersedness of information. Information is less available, not as easily accessible, as we would want it to be. (See <a href="../../2014/information-debt/">my earlier post on information debt</a> for some more thoughts on&nbsp;this.)</p>
<p>Not only that, the understanding of what and how we are testing, is equally dispersed. Strategy and operations are separated by the implicit tactics of test design techniques. In the test operations the middle part of the <span class="caps">OODA</span>-loop, orientation and decision, have been separated from the other two elements, observation and action. The first two are part of test design; the latter two of test execution. And in fact the observation is strongly directed by test design. So only the action as such (marking the test case as passed or failed) happens completely inside the test execution&nbsp;activities.</p>
<p>All in all this reminds me strongly of the Chinese room, a thought experiment by John Searle. A man is sitting in a room and he receives pieces of paper with Chinese characters on them. He has a big book with rules about what Chinese characters he has to write in response, depending on the pieces of paper he receives. Now, in fact, the pieces he receives contain questions and the characters he writes down are the correct answers. To an outside observer it would look that the person inside the room knows Chinese, yet this is not the case. So the question is: where is the knowledge, the understanding of Chinese? It&#8217;s not in the man and it&#8217;s not in the book. A possible answer is that the understanding resides in the system as a whole, in the man together with the&nbsp;book.</p>
<p>A similar argument can be made about testing based on test cases. It&#8217;s impossible to point at one thing or person that understands the whole: from test strategy to tactics to planned and executed operations. This understanding is present however in the complete system consisting of test strategy, test design techniques, test coverage matrix, test cases, test results and people. If this is a problem or not will depend on how we evaluate the complexity of the information problem that is testing. With the ironic twist that the bigger we estimate the complexity, the more necessary but also the more difficult it will be to avoid this dispersedness of information - or at least limit it&nbsp;sufficiently.</p>
<hr>
<p><em>This post was originally published on my old&nbsp;blog.</em></p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../../categories/epistemology/" rel="tag">epistemology</a></li>
            <li><a class="tag p-category" href="../../../categories/information-debt/" rel="tag">information&nbsp;debt</a></li>
            <li><a class="tag p-category" href="../../../categories/test-cases/" rel="tag">test&nbsp;cases</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../../2014/joining-the-fray-on-iso-29119/" rel="prev" title="Joining the fray on ISO 29119">Previous&nbsp;post</a>
            </li>
            <li class="next">
                <a href="../test-automation-five-questions-leading-to-five-heuristics/" rel="next" title="Test automation - five questions leading to five heuristics">Next&nbsp;post</a>
            </li>
        </ul></nav></aside></article><!--End of body content--><footer id="footer">
            Contents Â© 2026 
<a href="https://creativecommons.org/licenses/by-nc/4.0/" rel="nofollow" target="_blank">
<img alt="Creative Commons BY-NC License" style="border-width:0;margin: 0px 0px 0px 0px" src="https://licensebuttons.net/l/by-nc/4.0/80x15.png"></a>
<a href="mailto:site@joep.slmail.me">Joep Schuurkes</a> - Powered by <a href="https://getnikola.com" rel="nofollow" target="_blank">Nikola</a> - Analytics by <a href="https://plausible.io/" rel="nofollow" target="_blank">Plausible</a>
            
        </footer>
</div>
</div>

        <script src="../../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>