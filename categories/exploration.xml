<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Joep Schuurkes (Posts about exploration)</title><link>https://j19sch.github.io/</link><description></description><atom:link href="https://j19sch.github.io/categories/exploration.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2022 &lt;a href="mailto:j19sch@hey.com"&gt;Joep Schuurkes&lt;/a&gt; 
&lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;
&lt;img alt="Creative Commons License" style="border-width:0;margin: 0px 0px 0px 0px" src="https://licensebuttons.net/l/by/4.0/80x15.png" /&gt;
&lt;/a&gt;
</copyright><lastBuildDate>Sun, 06 Feb 2022 16:27:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Solving Black Box Puzzle 31 with data analysis</title><link>https://j19sch.github.io/blog/2019/solving-black-box-puzzle-31-with-data-analysis/</link><dc:creator>Joep Schuurkes</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://twitter.com/workroomprds"&gt;James Lyndsay&lt;/a&gt; has created a number of amazing &lt;a href="http://blackboxpuzzles.workroomprds.com/"&gt;Black Box Puzzles&lt;/a&gt;: tiny applications that challenge you to figure out what they do. (You can support him in creating more of these at &lt;a href="https://www.patreon.com/workroomprds"&gt;his Patreon page&lt;/a&gt;.) Two of these Puzzles, &lt;a href="http://blackboxpuzzles.workroomprds.com/puzzle29/"&gt;29&lt;/a&gt; and &lt;a href="http://blackboxpuzzles.workroomprds.com/puzzle31/"&gt;31&lt;/a&gt;, not only have a GUI to explore, but also an API.&lt;/p&gt;
&lt;p&gt;And that gave me an idea. If you explore these Puzzles through their GUI, you start from the inputs. You try out different inputs in the hope of discovering a pattern in the outputs. And then that pattern feeds back into your exploration.&lt;br&gt;
With an API, however - and because of the nature of Puzzle 31 - it becomes easy to get the outputs for all possible combinations of inputs. Which means you can start your exploration from the outputs instead of the inputs.&lt;/p&gt;
&lt;p&gt;Before I tell you how and what I did, three important remarks.&lt;br&gt;
First of all, I will be spoiling the solution to the Puzzle in this blog post. So this is the right moment to go and solve &lt;a href="http://blackboxpuzzles.workroomprds.com/puzzle31/"&gt;Puzzle 31&lt;/a&gt; for yourself first. Or at least go play a bit with it, so you have an idea what the inputs and outputs are.&lt;br&gt;
Secondly, I had already solved the Puzzle through the GUI a few months ago. So it was more of a "Can I find the solution this way as well?" than a "Can I find the solution?" thing.&lt;br&gt;
Finally, the code and the spreadsheet I created (linked throughout, also available on GitHub &lt;a href="https://github.com/j19sch/blackbox-puzzle-31"&gt;here&lt;/a&gt;), are not very clean. I thought about tidying them up, but my two reasons for not doing so are (1) laziness; (2) the way they are now gives a more honest picture of what I did.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://j19sch.github.io/blog/2019/solving-black-box-puzzle-31-with-data-analysis/"&gt;Read more…&lt;/a&gt; (18 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>automation</category><category>black box puzzle</category><category>data analysis</category><category>exploration</category><category>python</category><guid>https://j19sch.github.io/blog/2019/solving-black-box-puzzle-31-with-data-analysis/</guid><pubDate>Sun, 28 Apr 2019 11:16:34 GMT</pubDate></item><item><title>Three arguments against the verification-validation dichotomy</title><link>https://j19sch.github.io/blog/2015/three-arguments-against-the-verification-validation-dichotomy/</link><dc:creator>Joep Schuurkes</dc:creator><description>&lt;div&gt;&lt;p&gt;Last week while talking with two colleagues, one of them mentioned the verification/validation thing. And I noticed it made me feel uneasy. Because I know well enough what is meant by the distinction, but on a practical level I simply can't relate to it. When I think about what I do as a software tester and how verification versus validation applies to it, nothing happens. Blank mind. Crickets. Tumbleweed.
So after giving it some thought, I present you with three arguments against the verification-validation dichotomy.&lt;/p&gt;
&lt;p&gt;First of course, we have the obligatory interlude of defining these two terms. A place to start is the Wikipedia page on &lt;a href="http://en.wikipedia.org/wiki/Software_verification_and_validation"&gt;Software verification and validation&lt;/a&gt;. Unfortunately it contains conflicting definitions, so if anyone cares enough, please do fix. Luckily there's also the general &lt;a href="http://en.wikipedia.org/wiki/Verification_and_validation"&gt;Verification and validation&lt;/a&gt; page of Wikipedia, which gives us (among others) the tl;dr version of the distinction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verification: &lt;em&gt;Are we building the product right?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Validation: &lt;em&gt;Are we building the right product?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally there's the &lt;a href="http://www.istqb.org/downloads/finish/20/145.html"&gt;ISTQB glossary v2.4&lt;/a&gt; that borrows from ISO 9000:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verification: &lt;em&gt;Confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Validation: &lt;em&gt;Confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now on to the three arguments.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://j19sch.github.io/blog/2015/three-arguments-against-the-verification-validation-dichotomy/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>exploration</category><category>semantics</category><category>verification and validation</category><guid>https://j19sch.github.io/blog/2015/three-arguments-against-the-verification-validation-dichotomy/</guid><pubDate>Tue, 24 Mar 2015 19:53:24 GMT</pubDate></item><item><title>Test cases, can't do 'm no more</title><link>https://j19sch.github.io/blog/2013/test-cases-cant-do-m-no-more/</link><dc:creator>Joep Schuurkes</dc:creator><description>&lt;div&gt;&lt;p&gt;Continuing the style of my previous blog post...&lt;/p&gt;
&lt;p&gt;Some days ago I found myself no longer able to think in test cases while testing. Of course, it's not as if I was using test design techniques to generate test cases one day and woke up the next day to find myself unable to do it anymore. But still, about a week ago I figured I had explored enough to be able to write down the test cases I wanted to execute and found myself staring at a blank page (well ok, empty Excel sheet) feeling alienated from what I was planning to do.&lt;/p&gt;
&lt;p&gt;So what do I mean when saying "thinking in test cases". Simply put, you take a piece of functionality, let a test design technique loose on it and there you go: a set of test cases to execute. Combine test design techniques over the different pieces of functionality as required and you're all covered test strategy-wise. Or that's the idea.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://j19sch.github.io/blog/2013/test-cases-cant-do-m-no-more/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>exploration</category><category>test cases</category><category>test management</category><guid>https://j19sch.github.io/blog/2013/test-cases-cant-do-m-no-more/</guid><pubDate>Sat, 06 Jul 2013 18:19:32 GMT</pubDate></item><item><title>Some thoughts after attending the 'Getting a Grip on Exploratory Testing' workshop</title><link>https://j19sch.github.io/blog/2012/some-thoughts-after-attending-the-getting-a-grip-on-exploratory-testing-workshop/</link><dc:creator>Joep Schuurkes</dc:creator><description>&lt;div&gt;&lt;p&gt;About two weeks ago I attended &lt;a href="http://www.workroom-productions.com/"&gt;James Lyndsay&lt;/a&gt;'s 'Getting a Grip on Exploratory Testing' workshop in Amsterdam. So it's about time to write something about it…&lt;/p&gt;
&lt;p&gt;Now one of the things I dislike about workshop blog posts is that people will say "It was great! And person X is such a good trainer!" without saying much about the content of the workshop. However, I find myself now writing this post and thinking: I shouldn't post a full summary of the workshop. Not that it would spoil too much for any future attendee: most of the workshop consists of exercises and discussion. But posting a summary of the workshop that James has put a lot of effort in to create, just doesn't feel right. So let me just say this: the workshop was great and James is such a good trainer! :-D&lt;/p&gt;
&lt;p&gt;Now that's out of the way, there are a few things from the workshop I'd like to share. Of course, the usual disclaimer applies: these are my thoughts on what was presented during the workshop. Any misrepresentations are my responsibility.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://j19sch.github.io/blog/2012/some-thoughts-after-attending-the-getting-a-grip-on-exploratory-testing-workshop/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>exploration</category><category>software testing</category><category>workshop</category><guid>https://j19sch.github.io/blog/2012/some-thoughts-after-attending-the-getting-a-grip-on-exploratory-testing-workshop/</guid><pubDate>Sun, 29 Apr 2012 17:55:30 GMT</pubDate></item><item><title>The irony of scripted testing</title><link>https://j19sch.github.io/blog/2012/the-irony-of-scripted-testing/</link><dc:creator>Joep Schuurkes</dc:creator><description>&lt;div&gt;&lt;p&gt;A bit over a week ago, &lt;a href="https://twitter.com/jamesmarcusbach/status/185816224075227137"&gt;James Bach posted on twitter&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"This video shows a nice simple contrast between heavy scripted testing and exploratory testing &lt;a href="http://youtu.be/PxTqjAwM2Pw"&gt;http://youtu.be/PxTqjAwM2Pw&lt;/a&gt;"&lt;br&gt;
- James Bach (@jamesmarcusbach) March 30, 2012&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So I watched the video, hoping to see something that would make me go 'Cool!', but instead I went 'Hmmm.'&lt;/p&gt;
&lt;p&gt;First let me say that this video does get a few things right:&lt;br&gt;
- Exploratory testing can be structured by using charters.&lt;br&gt;
- Exploratory testing allows you to easily change your test approach based on your test results.&lt;br&gt;
- Exploratory testing is very adaptable when confronted with inaccurate or changing requirements.&lt;br&gt;
Yet notice how the above only talks about exploratory testing, because for every thing the video gets right about exploratory testing, it gets something wrong about scripted testing – or rather about how scripted testing works in practice.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://j19sch.github.io/blog/2012/the-irony-of-scripted-testing/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>exploration</category><category>software testing</category><guid>https://j19sch.github.io/blog/2012/the-irony-of-scripted-testing/</guid><pubDate>Mon, 09 Apr 2012 21:16:17 GMT</pubDate></item></channel></rss>